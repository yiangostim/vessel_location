name: AIS Dry Bulk Vessel Scraper

on:
  schedule:
    # Run every 6 hours with shorter 10-minute sessions to avoid rate limiting
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      duration_minutes:
        description: 'Duration to scrape in minutes'
        required: false
        default: '10'
        type: string
      dwt_min:
        description: 'Minimum DWT'
        required: false
        default: '40000'
        type: string
      dwt_max:
        description: 'Maximum DWT'
        required: false
        default: '100000'
        type: string

env:
  AISSTREAM_API_KEY: ${{ secrets.AISSTREAM_API_KEY }}

jobs:
  scrape-ais-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install websockets pandas asyncio-mqtt python-dateutil pytz
        
    - name: Ensure data directory exists
      run: |
        mkdir -p ais_data
        
    - name: Download existing data from repository
      run: |
        echo "Checking for existing data files in repository..."
        if [ -f "ais_data/dry_bulk_vessels.csv" ]; then
          echo "✅ Found existing CSV: $(wc -l < ais_data/dry_bulk_vessels.csv) lines"
          echo "📁 Size: $(du -h ais_data/dry_bulk_vessels.csv | cut -f1)"
        else
          echo "ℹ️  No existing CSV found - will create new one"
        fi
        
        if [ -f "ais_data/vessel_database.json" ]; then
          vessel_count=$(grep -o '"mmsi"' ais_data/vessel_database.json | wc -l)
          echo "✅ Found vessel database: $vessel_count vessels"
        else
          echo "ℹ️  No existing vessel database found - will create new one"
        fi
        
    - name: Archive old data if CSV gets too large
      run: |
        if [ -f "ais_data/dry_bulk_vessels.csv" ]; then
          file_size=$(du -m ais_data/dry_bulk_vessels.csv | cut -f1)
          if [ $file_size -gt 50 ]; then  # If > 50MB
            echo "CSV is ${file_size}MB, archiving old data..."
            
            # Create archive directory
            mkdir -p ais_data/archives
            
            # Move current CSV to archive with timestamp
            timestamp=$(date +%Y%m%d_%H%M%S)
            mv ais_data/dry_bulk_vessels.csv "ais_data/archives/dry_bulk_vessels_${timestamp}.csv"
            
            # Compress the archive
            gzip "ais_data/archives/dry_bulk_vessels_${timestamp}.csv"
            
            echo "✅ Archived large CSV file"
          fi
        fi

    - name: Run AIS Data Collection
      env:
        DURATION_MINUTES: ${{ github.event.inputs.duration_minutes || '10' }}
        DWT_MIN: ${{ github.event.inputs.dwt_min || '40000' }}
        DWT_MAX: ${{ github.event.inputs.dwt_max || '100000' }}
      run: |
        echo "🚀 Starting AIS data collection..."
        echo "Duration: $DURATION_MINUTES minutes"
        echo "DWT Range: $DWT_MIN - $DWT_MAX"
        echo ""
        
        python analyze_ais_data.py
        
        echo ""
        echo "✅ AIS data collection completed"
      
    - name: Display collection results
      run: |
        echo ""
        echo "=== DATA COLLECTION RESULTS ==="
        
        if [ -f "ais_data/dry_bulk_vessels.csv" ]; then
          total_lines=$(wc -l < ais_data/dry_bulk_vessels.csv)
          echo "📊 Total position records: $((total_lines - 1))"
          echo "📁 File size: $(du -h ais_data/dry_bulk_vessels.csv | cut -f1)"
          
          echo ""
          echo "🚢 Most recent vessel positions:"
          tail -5 ais_data/dry_bulk_vessels.csv | while IFS=',' read timestamp mmsi vessel_name lat lon speed course rest; do
            echo "   $vessel_name ($mmsi): $lat,$lon at ${speed}kts"
          done
        else
          echo "❌ No CSV file found after collection"
        fi
        
        if [ -f "ais_data/vessel_database.json" ]; then
          vessel_count=$(grep -o '"mmsi"' ais_data/vessel_database.json | wc -l)
          echo ""
          echo "🗃️  Vessel database: $vessel_count unique vessels"
        fi

    - name: Commit and push data to repository
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "AIS Data Collector"
        
        # Check what files we have
        echo "📋 Checking data files..."
        if [ -d "ais_data" ] && [ "$(ls -A ais_data)" ]; then
          ls -la ais_data/
          
          # Stage the data files
          git add ais_data/
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "ℹ️  No new data to commit"
          else
            echo "💾 Committing new data..."
            git commit -m "🚢 Update AIS data - $(date -u '+%Y-%m-%d %H:%M:%S UTC') 
            
            Collection summary:
            - Duration: ${{ github.event.inputs.duration_minutes || '10' }} minutes
            - Target DWT: ${{ github.event.inputs.dwt_min || '40000' }}-${{ github.event.inputs.dwt_max || '100000' }}
            - Workflow: ${{ github.run_number }}"
            
            # Push to repository
            echo "📤 Pushing data to repository..."
            git push origin main
            echo "✅ Data successfully saved to repository"
          fi
        else
          echo "❌ ERROR: No data directory found or empty!"
          exit 1
        fi
        
    - name: Upload data as backup artifact
      uses: actions/upload-artifact@v4
      if: always()  # Upload even if previous steps failed
      with:
        name: ais-data-backup-${{ github.run_number }}
        path: ais_data/
        retention-days: 30
        
    - name: Print final summary
      if: always()
      run: |
        echo ""
        echo "🏁 WORKFLOW SUMMARY"
        echo "=================="
        echo "⏰ Started: $(date -d '@${{ github.event.repository.pushed_at }}' '+%Y-%m-%d %H:%M:%S UTC' 2>/dev/null || echo 'Unknown')"
        echo "🎯 Target: Dry bulk vessels ${{ github.event.inputs.dwt_min || '40000' }}-${{ github.event.inputs.dwt_max || '100000' }} DWT"
        echo "⏱️  Duration: ${{ github.event.inputs.duration_minutes || '10' }} minutes"
        echo "🔄 Run #: ${{ github.run_number }}"
        echo "🌐 Next scheduled run: $(date -d '+6 hours' '+%Y-%m-%d %H:%M:%S UTC')"
        echo ""
        echo "📊 Data persistence strategy:"
        echo "   • Appends to existing CSV in repository"
        echo "   • Commits changes back to main branch" 
        echo "   • Archives large files (>50MB) automatically"
        echo "   • Creates backup artifacts for 30 days"
