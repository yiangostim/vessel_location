name: AIS Dry Bulk Vessel Scraper

on:
  schedule:
    # Run every 6 hours with shorter 10-minute sessions to avoid rate limiting
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      duration_minutes:
        description: 'Duration to scrape in minutes'
        required: false
        default: '10'
        type: string
      dwt_min:
        description: 'Minimum DWT'
        required: false
        default: '40000'
        type: string
      dwt_max:
        description: 'Maximum DWT'
        required: false
        default: '100000'
        type: string

env:
  AISSTREAM_API_KEY: ${{ secrets.AISSTREAM_API_KEY }}

jobs:
  scrape-ais-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install websockets pandas asyncio-mqtt python-dateutil pytz
        
    - name: Create data directory
      run: |
        mkdir -p ais_data
        
    - name: Archive old data if CSV gets too large
      run: |
        if [ -f "ais_data/dry_bulk_vessels.csv" ]; then
          file_size=$(du -m ais_data/dry_bulk_vessels.csv | cut -f1)
          if [ $file_size -gt 50 ]; then  # If > 50MB
            echo "CSV is ${file_size}MB, archiving old data..."
            
            # Create archive directory
            mkdir -p ais_data/archives
            
            # Move current CSV to archive with timestamp
            timestamp=$(date +%Y%m%d_%H%M%S)
            mv ais_data/dry_bulk_vessels.csv "ais_data/archives/dry_bulk_vessels_${timestamp}.csv"
            
            # Compress the archive
            gzip "ais_data/archives/dry_bulk_vessels_${timestamp}.csv"
            
            echo "Archived large CSV file"
          fi
        fi

    - name: Run AIS Scraper
      env:
        DURATION_MINUTES: ${{ github.event.inputs.duration_minutes || '10' }}
        DWT_MIN: ${{ github.event.inputs.dwt_min || '40000' }}
        DWT_MAX: ${{ github.event.inputs.dwt_max || '100000' }}
      run: python analyze_ais_data.py
      
    - name: Display data collection summary
      run: |
        echo "=== DATA COLLECTION SUMMARY ==="
        echo "Collection frequency: Every 6 hours (optimal for 12-15kt vessels)"
        echo "Session duration: 10 minutes per run"
        echo ""
        if [ -f "ais_data/dry_bulk_vessels.csv" ]; then
          total_lines=$(wc -l < ais_data/dry_bulk_vessels.csv)
          echo "ðŸ“Š Main CSV file: $((total_lines - 1)) position records"
          echo "ðŸ“ File size: $(du -h ais_data/dry_bulk_vessels.csv | cut -f1)"
          echo "ðŸ“… Data age: $(stat -c %y ais_data/dry_bulk_vessels.csv | cut -d' ' -f1) to $(date +%Y-%m-%d)"
          echo ""
          echo "ðŸš¢ Latest vessel positions (last 3):"
          tail -3 ais_data/dry_bulk_vessels.csv | while read line; do
            echo "   $line"
          done
        else
          echo "âŒ No CSV file found"
        fi
        
        if [ -f "ais_data/vessel_database.json" ]; then
          vessel_count=$(grep -o '"mmsi"' ais_data/vessel_database.json | wc -l)
          echo ""
          echo "ðŸ—ƒï¸  Vessel database: $vessel_count unique vessels tracked"
        fi
        
        echo ""
        echo "â„¹ï¸  Data Collection Strategy:"
        echo "   â€¢ Every 6 hours = ~25-30nm vessel movement between samples"  
        echo "   â€¢ 10min sessions = captures multiple position updates per vessel"
        echo "   â€¢ Appends new data, never deletes historical positions"
        echo "   â€¢ Tracks dry bulk carriers 40,000-100,000 DWT globally"

    - name: Commit and push data
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action AIS Scraper"
        
        echo "Current git status:"
        git status
        
        # Force sync with remote
        git fetch origin main || echo "Fetch failed, continuing..."
        git reset --hard origin/main || echo "Reset failed, continuing..."
        
        # Check if data directory exists and has content
        if [ -d "ais_data" ] && [ "$(ls -A ais_data)" ]; then
          echo "Data directory contents:"
          ls -la ais_data/
          
          # Re-add the data files
          git add ais_data/ || echo "Git add failed"
          
          if [ -n "$(git status --porcelain)" ]; then
            echo "Changes detected, committing..."
            git commit -m "Update AIS data - $(date '+%Y-%m-%d %H:%M:%S UTC')" || echo "Commit failed"
            git push origin main || echo "Push failed"
            echo "Data commit attempted"
          else
            echo "No changes detected in git status"
          fi
        else
          echo "ERROR: No ais_data directory or it's empty!"
        fi
        
    - name: Upload data as artifact
      uses: actions/upload-artifact@v4
      with:
        name: ais-data-backup-${{ github.run_number }}
        path: ais_data/
        retention-days: 90
